<!doctype html><html><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><title>
        On the optimal memory allocation for Random Walks on a Directed Graph (Draft)
    </title><link href=/corro.svg rel=icon type=image/png><link href=https://zom.wtf/fonts.css rel=stylesheet><link href=/path/to/folder/css/academicons.min.css rel=stylesheet><link href=https://zom.wtf/atom.xml rel=alternate title=zom.wtf type=application/atom+xml><link href=https://zom.wtf/theme/light.css rel=stylesheet><link href=https://zom.wtf/main.css media=screen rel=stylesheet><body><div class=content><header><div class=main><img id=title_icon src=/corro.svg><a href=https://zom.wtf>zom.wtf</a><div class=socials><a class=social href=https://twitter.com/zommiommy> <img alt=twitter src=/social_icons/twitter.svg> </a><a class=social href=https://github.com/zommiommy> <img alt=github src=/social_icons/github.svg> </a></div></div><nav><a href=/posts style=margin-left:.7em>/posts</a><a href=/about style=margin-left:.7em>/about</a><a href=/goodstuff style=margin-left:.7em>/good_stuff</a></nav></header><main><article><div class=title><div class=page-header>On the optimal memory allocation for Random Walks on a Directed Graph (Draft)</div><div class=meta>Posted on <time>2021-08-31</time></div></div><h1>Table of Contents</h1><ul><li><a href=https://zom.wtf/posts/random-walk-allocation/#extimate-the-parameter-p>Extimate the parameter \(p\)</a><li><a href=https://zom.wtf/posts/random-walk-allocation/#average-random-walk-length>Average random walk length</a><li><a href=https://zom.wtf/posts/random-walk-allocation/#choosing-the-optimal-precision-beta>Choosing the Optimal precision \(\beta\)</a> <ul><li><a href=https://zom.wtf/posts/random-walk-allocation/#time-and-memory-analysis>Time and Memory Analysis</a></ul><li><a href=https://zom.wtf/posts/random-walk-allocation/#memory-analysis>Memory analysis</a> <ul><li><a href=https://zom.wtf/posts/random-walk-allocation/#finding-the-optimal-beta-pt-1>Finding the optimal \(\beta\) pt.1</a><li><a href=https://zom.wtf/posts/random-walk-allocation/#finding-the-optimal-beta-pt-2>Finding the optimal \(\beta\) pt.2</a><li><a href=https://zom.wtf/posts/random-walk-allocation/#what-beta-to-use-and-when>What \(\beta\) to use and when</a></ul><li><a href=https://zom.wtf/posts/random-walk-allocation/#time-analysis>Time analysis</a><li><a href=https://zom.wtf/posts/random-walk-allocation/#time-with-beta-optimized-for-memory>Time with \(\beta\) optimized for memory.</a></ul><section class=body><p>In this post I will share some simple results around the optimal memory allocation for a random walk on a directed graph, and its time-memory tradeoffs. I studied this to speed-up the computation of random-walks by optimally guessing how much memory to allocate for each walk.<p>To precisely define what optimality means in this context, we have to setup our cost framework. We are going to study a simplified allocator which by default allocates arrays of length \(k\) and if needed, it expand the array to the maximum length \(L\), and possibly copy the old values to the new array (for a total cost of \(k + L\)).<p>Therefore we can define the cost function for a random walk of length \(l\) as:<p>\[m(l) = \left\{\begin{matrix}k & l \le k \\ k + L & else\end{matrix}\right.\]<p>Therefore the expected cost is:<p>\[\mathbb{E}[m(k)] = k P(X \le k) + (k + L) P(X > k) \]<p>Where \(X\) is the aleatory variable associated to the length of a random walk on a given graph.<p>These probabilities depends on several factors and are hard to analyze, so to be able to gain some insight on this complex problem, we are going to follow <a href=https://i.redd.it/qrywf2y4e6u31.jpg>the first Engineering rule</a> and approximate it.<p>The idea is that at each step of the random walk the random walk will either stop or continue so we can <strong>approximate</strong> it as a Bernulli trial.<p>Therefore, are going to use a simplified model where we have a fully connected graph except for traps nodes (in blue in the image), which by definition are nodes with only inbound edges.<p><img alt=test src=/model_graph_trap.svg><p>They are called trap nodes because once the random walk it can no longer continue, because the node doesn't have any outbound edges.<p>Using this simplified model, each step is a bernulli trial, and we are intrested in the probability of a sequence of \(k\) successes and a failure which by definition is a Geometric distribution of parameter \(p\).<p>Which mean that we can approximate the terms of the previous formula as: \[P(X \le k) \approx (1 - p)^k p \qquad P(X > k) \approx (1 - p)^k\]<p>\[\mathbb{E}[m(k)] \approx k (1 - p)^k p + (k + L) (1 - p)^k \]<p>since we want to minimize the expected cost, we can compute it's partial derivate to find the critical points:<p>\[\frac{\partial}{\partial k}\mathbb{E}[m(k)] \approx p(1-p)^k \big(k \log(1-p) + 1 \big) + (1-p)^k \big ((L + k) \log(1 - p) + 1 \big)\]<h2 id=extimate-the-parameter-p>Extimate the parameter \(p\)</h2><p>\(p\), it's the probability that during a walk, the successor of a given node it's a trap.<p>This is expensive to compute <strong>during the walk</strong> so we might approximate it computing the <strong>Average</strong> probability over all nodes.<p>For a given node \(n\), with neighbours \(N(n)\), the campionary probability for the node to have traps as neighbours is:<p>\[P(\text{Node \(n\) has a trap neigbour}) = \frac{1}{|N(n)|} \sum_{d \in N(n)} \text{is_trap}(d)\]<p>To have the probability that on any given graph a node has a trap as a neighbouring node, we need to compute the mean:<p>\[ \mathbb{E}[P(\text{Node $n$ has a trap neigbour})]= \frac{1}{|N|}\sum_{n\in N}\frac{1}{|N(n)|}\sum_{d\in N(n)} \text{is_trap}(d) \]<p>For node_to_vec we can use:<p>\[ \mathbb{E}[P(\text{Node $n$ has a trap neigbour})]= \frac{1}{|N|}\sum_{n\in N}\sum_{n_i\in {Ne}_n} P_i \quad \text{is_trap}(d) \]<p>Fs is the Forward-Star aka the neighbours of the given node.<p>We can use this value to compute the expected length of a random walk.<h2 id=average-random-walk-length>Average random walk length</h2><p>We need to compute the average length until a trap, therefore we need a geometric distribution.<p>$$P(X = k) = p(1-p)^k$$<p>$$P(X \le k) = 1 - (1-p)^k$$<p>Therefore, we can choose a "safety level" \(\beta\) which it's an user chosen parameter.<p>So, given an estimate of \(p\), we can compute the random walk length \(k\) associated to the safety level \(\beta\):<p>$$k = \frac{\ln(1 - \beta)}{\ln\left(1-p\right)}$$<h2 id=choosing-the-optimal-precision-beta>Choosing the Optimal precision \(\beta\)</h2><h3 id=time-and-memory-analysis>Time and Memory Analysis</h3><p>We want to model the memory and time cost of an allocation strategy for the walks of a given graph.<p>The simple strategy we will analyze is just to allocate \(k\) elements, and then if the walk excede this size, we reallocate the array with \(L\) elements which is the max length of walks.<p>Reallocating is an expensive operation since (on the current libc implemnetation of jmalloc) if the next heap chunck it's occupied, the allocator is forced to allocate a new array and copy there the content and free the old array.<p>We denote \(s_i\) is the length of the i-th walk. Denoting with \(m(s_i)\) the memory required by the i-th walk.<h2 id=memory-analysis>Memory analysis</h2><p>Since the allocation strategy allocate \(k\) elements and then, if the walk exceeds \(k\) steps, we expand it to \(L\), it's memory usage can be modeled as:<p>\[ m(s_i) = \left\{\begin{matrix}k & s_i \le k\\L & else\end{matrix}\right.\]<p>$$M(k) = \sum_i m(s_i)$$<p>$$M(s) = \sum_i k + \sum_j L = w(1-\beta) k + w\beta L$$<p>$$m(s) = (1-\beta) \frac{\ln(1 - \beta)}{\ln\left(1-p\right)} + \beta L$$<p>$$m(s) = \frac{1}{\ln(1-p)} \left[(1-\beta)\ln(1 - \beta) + \beta \ln\left((1-p)^L\right)\right]$$<p>$$m(s) = \frac{1}{\ln(1-p)} \left[(1-\beta)\ln(1 - \beta) \underbrace{+\beta\ln(\beta) -\beta\ln(\beta)}_0 + \beta \ln\left((1-p)^L\right)\right]$$<p>$$m(s) = \frac{1}{\ln(1-p)} \left[- H(\beta) -\beta\ln(\beta) + \beta \ln\left((1-p)^L\right)\right]$$<p>$$m(s) = \frac{1}{-\ln(1-p)} \left[ H(\beta) + \beta\ln(\beta) - \beta \ln\left((1-p)^L\right)\right]$$<p>$$m(s) = \frac{1}{-\ln(1-p)} \left[H(\beta) + \beta \ln\left(\frac{\beta}{(1-p)^L}\right)\right]$$<p>Since it's the entropy of a binary event, \(0 \le H(\beta) \le 1\) so we can use 1 as upperbound.<p>$$m(s) \le \frac{1}{-\ln(1-p)} \left[1 + \beta \ln\left(\frac{\beta}{(1-p)^L}\right)\right]$$<p>Therefore the <strong>final complexity</strong> with regards to \(\beta\) is:<p>$$m(s) = \mathcal{O}\left( \beta \ln\left(\frac{\beta}{(1-p)^L}\right)\right) = \mathcal{O}\left( \beta \ln\left(\beta\right)\right)$$<h3 id=finding-the-optimal-beta-pt-1>Finding the optimal \(\beta\) pt.1</h3><p>It's to notice that since \(0 \le \beta \le 1\):<p>$$ -\frac{1}{e} \le \beta \ln(\beta) \le 0$$<p>and the value of \(\beta\) that minimize \(\beta \ln(\beta)\) is \(\beta = \frac{1}{e}\).<p>So we have the result:<p>$$m(s) \le \frac{1}{-\ln(1-p)} \left[1 + \beta \ln\left(\frac{\beta}{(1-p)^L}\right)\right]$$<p>$$m(s) \le \frac{1}{-\ln(1-p)} \left[1 + \beta \ln\left(\beta\right) - \beta \ln((1-p)^L)\right]$$<p>$$m(s) \le \frac{1}{-\ln(1-p)} \left[1 -\frac{1}{e} - \frac{1}{e} \ln((1-p)^L)\right]$$<p>$$m(s) \le \frac{(1 - \frac{1}{e}) - \frac{1}{e} L \ln(1-p)}{-\ln(1-p)}$$<p>$$m(s) \le \frac{1}{e} L + \frac{1 - \frac{1}{e}}{-\ln(1-p)}$$<h3 id=finding-the-optimal-beta-pt-2>Finding the optimal \(\beta\) pt.2</h3><p>Another assignement of \(\beta\) is \(\beta = (1-p)^L\) then we get<p>$$m(s) = \frac{1}{-\ln(1-p)} \left[H(\beta) + \beta \ln\left(\frac{\beta}{\beta}\right)\right]$$<p>$$m(s) = \frac{H(\beta)}{-\ln(1-p)} \le \frac{1}{-\ln(1-p)}$$<p>The unit of measure is number of integers. Therefore if we multiply the bound by 64 we get the size in bits for modern systems.<h3 id=what-beta-to-use-and-when>What \(\beta\) to use and when</h3><p>Let's find invert the disequation to know when to use which equation<p>$$ \frac{1}{e} L + \frac{1 - \frac{1}{e}}{-\ln(1-p)} \le \frac{1}{-\ln(1-p)}$$<p>$$ \frac{1}{e} L \le \frac{1}{-\ln(1-p)} - \frac{1 - \frac{1}{e}}{-\ln(1-p)}$$<p>$$ \frac{1}{e} L \le \frac{1 - (1 - \frac{1}{e})}{-\ln(1-p)} $$<p>$$ \frac{1}{e} L \le \frac{\frac{1}{e}}{-\ln(1-p)} $$<p>$$ L \le \frac{1}{-\ln(1-p)} $$<p>$$ -\ln(1-p)L \le 1 $$<p>$$ \ln\left(\frac{1}{(1-p)^L}\right) \le 1 $$<p>$$ \frac{1}{(1-p)^L} \le e $$<p>$$ (1-p)^L > \frac{1}{e} $$<p>Therefore if \((1-p)^L > \frac{1}{e}\) then it's best to use \(\beta = \frac{1}{e}\) otherwise \(\beta = (1-p)^L\).<p>Therefore:<p>$$\beta = \left\{\begin{matrix} \frac{1}{e} & (1-p)^L > \frac{1}{e}\ (1-p)^L & (1-p)^L \le \frac{1}{e} \end{matrix}\right.$$<p>Which is equivalent to:<p>$$\beta = \min{\left\{\frac{1}{e}, (1-p)^L\right\}}$$<p>Therefore almost we will have \(\beta = (1-p)^L\) since \(\frac{1}{e} \approx 0.36787944\).<p>Moreover, it's easy to choose based on \(p\), since the walk length we expect is around 80: $$p = 1 -\sqrt[L]{\frac{1}{e}}$$ and $$p = 1 -\sqrt[80]{\frac{1}{e}} = 1 - 0.9875778 = 0.0124222$$<p>Finally, for the expected walk length, if the trap_rate is over 1.24% then it's better to use \(\beta = (1-p)^L\) .<p>The final result is that the optimal value is:<p>$$\beta = \min{\left\{\frac{1}{e}, (1-p)^L\right\}}$$<h2 id=time-analysis>Time analysis</h2><p>Since the allocation strategy allocate \(k\) elements and then, if the walk exceeds \(k\) steps, we expand it to \(L\), it's memory usage can be modeled as:<p>$$ t(s_i) = \left\{\begin{matrix} k & s_i \le k\\ L + k & else \end{matrix}\right. $$<p>$$T(k) = \sum_i t(i)$$<p>We can greately simplify this formulations:<p>$$T(k) = \sum_i k + \sum_j (k + L) = w(1 - \beta) k + w\beta(k + L)$$<p>$$t(k) = (1 - \beta) \frac{\ln(1 - \beta)}{\ln\left(1-p\right)} + \beta\left(\frac{\ln(1 - \beta)}{\ln\left(1-p\right)} + L\right)$$<p>$$t(k) = \frac{1}{\ln\left(1-p\right)} \left[(1 - \beta) \ln(1 - \beta)+ \beta\ln(1 - \beta) + L\beta\ln\left(1-p\right)\right]$$<p>$$t(k) = \frac{1}{\ln\left(1-p\right)} \left[(1 - \beta) \ln(1 - \beta) + \underbrace{\beta \ln(\beta) - \beta \ln(\beta)}_0 + \beta\ln(1 - \beta) + L\beta\ln\left(1-p\right)\right]$$<p>$$t(k) = \frac{1}{\ln\left(1-p\right)} \left[-H(\beta) - \beta \ln(\beta) + \beta\ln(1 - \beta) + L\beta\ln\left(1-p\right)\right]$$<p>$$t(k) = \frac{1}{\ln\left(1-p\right)} \left[-H(\beta) + \beta\ln\left(\frac{1 - \beta}{\beta}\right) + \beta\ln\left((1-p)^L\right)\right]$$<p>$$t(k) \le \frac{1}{\ln\left(1-p\right)} \left[1 + \beta\ln\left(\frac{1 - \beta}{\beta}\right) + \beta\ln\left((1-p)^L\right)\right]$$<h2 id=time-with-beta-optimized-for-memory>Time with \(\beta\) optimized for memory.</h2><p>if \(\beta = \frac{1}{e}\)<p>$$t(k) \le \frac{1}{e}L + \frac{1.19912}{\ln(1-p)}$$<p>if \(\beta = (1-p)^L\)<p>$$t(k) \le \frac{1}{\ln(1-p)} + \frac{(1-p)^L \ln(1 - (1-p)^L)}{\ln(1-p)} \le \frac{1}{\ln(1-p)} + \frac{(1-p)^L}{\ln(1-p)}$$<p>$$t(k) \le \frac{1}{\ln(1-p)} + \frac{(1-p)^L}{\ln(1-p)} \le \frac{2}{\ln(1-p)}$$</section></article></main></div><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script async id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>